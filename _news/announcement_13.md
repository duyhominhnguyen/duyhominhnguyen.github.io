---
layout: post
date: 2025-09-26 00:00:00-0000
inline: true
related_posts: false
---

ðŸ”” Excited to share that our works on (i) **[ExGra-Med](https://exgra-med.github.io/)** â€” a *data-efficient multimodal large language model (LLM) for healthcare*; 
(ii) **[Token Redundancy in 3D Point Cloud Transformers](https://openreview.net/pdf?id=uGO1tgU3Mc)** â€” uncovering how existing 3D transformers (e.g., Ptv-3, Sonata) are **over-tokenized**, and proposing an efficient *token merging* strategy that reduces computation by up to **90-95%** while preserving accuracy; and
(iii) **[Over-Optimization in RLHF for LLM Post-Training](https://arxiv.org/pdf/2506.08681)** â€” exploring how **reinforcement learning from human feedback** can lead to alignment instability and proposing new insights into optimization LLM post-training have been accepted to **[NeurIPS 2025]**(https://neurips.cc/) ðŸŽ‰. Excited to present and discuss them at San Diego ðŸš€