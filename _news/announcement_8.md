---
layout: post
date: 2025-02-20 00:00:00-0000
inline: true
related_posts: false
---

:bell: Excited to share our latest work! ðŸŽ‰

(i) [On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation](https://arxiv.org/pdf/2502.03029) â€“ We introduce a Mixture of Experts (MoE) perspective to explain the mechanism behind LLaMA-Adapterâ€™s prompt learning.

(ii) [MGPath](https://arxiv.org/pdf/2502.07409) â€“ A novel multi-granular prompt learning method for few-shot WSI pathology prediction, leveraging the power of foundation vision-language models.