---
layout: post
date: 2025-11-08 00:00:00-0000
inline: true
related_posts: false
---

ðŸ”” **Exciting News!**

Weâ€™re thrilled to share that our two recent works have been **accepted to [AAAI 2026](https://aaai.org/conference/aaai/aaai-26/)** â€” one as an **oral** and the other as a **poster** presentation! ðŸŽ‰  

i. **Multi-Mood** â€” a multi-modal large language model that integrates **video, audio, and text** with **psychological criteria** through **reinforcement learning** to enable *trustworthy and emotionally aligned responses*.  

ii. **LIBERO-Mem** â€” a **non-Markovian task suite** for **short- and long-horizon object tracking and manipulation**, featuring **temporally sequenced subgoals** that challenge models to reason *beyond the current observation*.  

ðŸ“„ **Papers** and **Codes** will be released soon ðŸŽ‰ â€” stay tuned!
